# -*- coding: utf-8 -*-
"""diabetesMLtest.ipynb
Automatically generated by Colaboratory. """

"""Lesson: https://www.youtube.com/watch?v=VtRLrQ3Ev-U
   Tutor: Kylie Ying """

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow_hub as hub
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import RandomOverSampler


df = pd.read_csv("diabetes.csv") #read data via pandas
df.head() #see dataFrame

df[df['Outcome'] == 0] #all outcomes that = 0 (Diabetes Negative)

df[df['Outcome'] == 1] #all outcomes that = 1 (Diabetes Positive)

#for i in range(len(df.columns[:-1])): #display data for each column in data
  #label = df.columns[i]
  #plt.hist(df[df['Outcome'] == 1][label], color = 'blue', label = 'Diabetes Pos', alpha = 0.7, density = True, bins = 15)
  #plt.hist(df[df['Outcome'] == 0][label], color = 'red', label = 'Diabetes Neg', alpha = 0.7, density = True, bins = 15) #alpha makes it easier to see variations
  #plt.title(label)
  #plt.ylabel('Probability')
  #plt.xlabel(label)
  #plt.legend()
  #plt.show()

print("Diabetes Negative: {}".format(len(df[df['Outcome'] == 0]))) #prints num of neg- patients
print("Diabetes Positive: {}".format(len(df[df['Outcome'] == 1]))) #prints num of pos+ patients

x = df[df.columns[:-1]].values #applies values into x and y variables
y = df[df.columns[-1]].values

scalar = StandardScaler() #removes the mean and scales to unit variance ~check out: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html
                          #aka: balances data for analysis
x = scalar.fit_transform(x) #fits to data then transforms it

#data = np.hstack((x, np.reshape(y, (-1, 1)))) #reshapes Y into a 2D matrix
#transformed_df = pd.DataFrame(data, columns=df.columns) #creates a new dataframe with new/refined values

over = RandomOverSampler() #balances dataset ~check out: https://imbalanced-learn.org/dev/references/generated/imblearn.over_sampling.RandomOverSampler.html
x, y = over.fit_resample(x,y)
data = np.hstack((x, np.reshape(y, (-1, 1))))
transformed_df = pd.DataFrame(data, columns=df.columns)

#len(transformed_df[transformed_df['Outcome'] == 1]), len(transformed_df[transformed_df['Outcome'] == 0])
#^^ returns number of data on both sides

x_train, x_temp, y_train, y_temp = train_test_split(x, y, test_size=0.4, random_state = 0) #0.4 = 40% of dataset
x_valid, x_test, y_valid, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=0) #0.5 = 50% of 40% of dataset

model = tf.keras.Sequential([
                              tf.keras.layers.Dense(16, activation='relu'), 
                              tf.keras.layers.Dense(16, activation='relu'),
                              tf.keras.layers.Dense(1, activation="sigmoid")
])
#Sequential Data: used for when points are dependent on other points in the data set
#for more infor on Sequential data: https://medium.com/analytics-vidhya/sequential-data-and-the-neural-network-conundrum-b2c005f8f865
#Dense: each neuron per layer recieves all previous neuron data from prev layer
#Relu + Sigmoid: weighs outputs to be either 0 or 1 ~check out: https://dhruvs.space/posts/ml-basics-issue-4/#relu-what-is-it

#create the learning model
model.compile(
              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy']
)

#evaluates the performance of the model on the given datasets
model.evaluate(x, y)
model.evaluate(x_train, y_train)
model.evaluate(x_valid, y_valid)


model.fit(x_train, y_train, batch_size=16, epochs=20, validation_data=(x_valid, y_valid))

#final evaluation to check performance on test-data (unseen data to the model)
model.evaluate(x_test, y_test)
